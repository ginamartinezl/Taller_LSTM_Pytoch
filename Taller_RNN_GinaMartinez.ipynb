{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gina Martinez Lopez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2uDB30LdMr4"
      },
      "source": [
        "# Character-Level LSTM in PyTorch\n",
        "\n",
        "Construiremos una red neuronal recurrente tipo LSTM en PyTorch. La red entrenará caracter por caracter en un texto, luego generará texto nuevo caracter por caracter. Entrenamremos con un texto de un libro o novela que usted haya leído. Puede usar, por ejemplo [ESTE](https://www.gutenberg.org/) recurso para buscar un texto en archivo .txt. El texto generado estará basado en dicho libro.\n",
        "\n",
        "\n",
        "Referencias: Andrej Karpathy [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) y [implementation in Torch](https://github.com/karpathy/char-rnn).\n",
        "\n",
        "El ejercicio consiste en realizar las instrucciones marcadas como TO_DO. Entregar a más tardar el 30 de marzo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fz2txRndMr_"
      },
      "source": [
        "Importamos las librerías y paquetes requeridos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "p3UD6qhAdMsA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqmQIcvzdMsB"
      },
      "source": [
        "## Cargar los datos\n",
        "\n",
        "Cargamos el texto en un archivo .txt y lo convertimos en enteros para que la red lo use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "gpADs2vkdMsB"
      },
      "outputs": [],
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('/content/AdanyEva.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nUo5TyvJaG8I"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnZE-dGedMsC"
      },
      "source": [
        "Imprimimos los primeros 100 characteres y revisamos que el texto luzca bien."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ZENWOkLdMsC",
        "outputId": "35ac4b3d-e770-4107-d06b-0b8e7efbe048"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Adán, Padre de los Hombres, fue creado en el día 28 de octubre, a las\\ndos de la tarde... Afírmalo as'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQvwMu-sdMsC"
      },
      "source": [
        "### Tokenización\n",
        "\n",
        "En las celdas siguientes, creamos un par de diccionarios para convertir los caracteres desde y hacia enteros. Esto los vuelve más fáciles de usar como entrada de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "if-65nUDdMsD"
      },
      "outputs": [],
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB6dVD7dMsD"
      },
      "source": [
        "Imprimamos los caracteres anteriores, pero ahora codificados como enteros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfsKAU-YdMsE",
        "outputId": "b23faabd-e028-4269-d49d-f694511aa062"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 37,  76,  17,  71, 107,  72,  57,  62,  76,  49,   0,  72,  76,\n",
              "         0,  72,  38,  77,  90,  72,  92,  77,  96,  18,  49,   0,  90,\n",
              "       107,  72,  58,  39,   0,  72,  33,  49,   0,  62,  76,  77,  72,\n",
              "         0,  71,  72,   0,  38,  72,  76,   5,  62,  72,  30,  23,  72,\n",
              "        76,   0,  72,  77,  33,  91,  39,  18,  49,   0, 107,  72,  62,\n",
              "        72,  38,  62,  90,  25,  76,  77,  90,  72,  76,   0,  72,  38,\n",
              "        62,  72,  91,  62,  49,  76,   0,  60,  60,  60,  72,  37,  58,\n",
              "         5,  49,  96,  62,  38,  77,  72,  62,  90])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al5ptl4bdMsE"
      },
      "source": [
        "## Pre-procesamiento de los datos\n",
        "\n",
        "Nuestra red LSTM espera una entrada codificada con **one hot encoding**. Definimos una función para esto.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "X6QXqA8DdMsE"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    # Initialize the encoded array\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd4obwVQdMsE",
        "outputId": "bcfc9f11-2465-4961-a394-0cdca48d9688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ]
        }
      ],
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUC6IT-dMsF"
      },
      "source": [
        "## Crear mini-batches de entrenamiento\n",
        "\n",
        "Para entrenar en este texto, queremos crear mini lotes.\n",
        "\n",
        "En este ejemplo, tomaremos los caracteres codificados (los pasamos como parámetro `arr`) y los separaremos en múltiples trozos o sequencias. A saber, el *número de trozos* que se ingresa a la red es `batch_size` (tenga en cuenta que no es equivalente al batchsize que usábamos antes). Cada sequencia ingresada a la red tendrá tamaño `seq_length` (éste sí es equivalente al batch size anterior).\n",
        "\n",
        "### Creando los lotes\n",
        "\n",
        "**Lo primero que debemos hacer es descartar algún texto, de manera que sólo tengamos mini lotes de tamaño completo.**\n",
        "\n",
        "Cada lote contiene $N \\times M$ caracteres, en donde $N$ es el número de secuencias en cada lote, y $M$ es seq_length o el número de pasos de tiempo en una secuencia. Luego, para obtener el número total $K$ de lotes, podemos dividir la longitud de `arr`, entre el número de caracteres por lote. El número total de caracteres que usaremos del texto sería entonces $N * M * K$. Lo que sobre, deberíamos descartarlo.\n",
        "\n",
        "**Luego de eso, necesitamos separar todo el texto que usaremos `arr` en $N$ trozos.**\n",
        "\n",
        "Puede hacer esto usando `arr.reshape(size)`, en donde `size` es una tupla con las dimensiones del arreglo re-dimensionado. Sabemos que queremos $N$ trozos (líneas) con todo el texto, entonces que esa sea la primera dimensión. Para la segunda dimensión podemos escribir directamente su tamaño, o usar el valor `-1`. Éste llenará el arreglo con el tamaño apropiado, dependiento de la cantidad de entradas disponibles.\n",
        "\n",
        "**3. Ahora que tenemos este arreglo, podemos iterar a través de él para obtener nuestos lotes con los cuales alimentar la red.**\n",
        "\n",
        "La idea es que cada lote sea una ventana de tamaño $N \\times M$, del gran arreglo de tamaño $N \\times (M * K)$. Para cada lote siguiente, la ventana se mueve una distancia de `seq_length`.\n",
        "\n",
        "Adicionalmente queremos crear arreglos de entrada y arreglos de etiquetas.Recuerde que las etiquetas són las mismas entradas pero desfasadas por un caracter.\n",
        "\n",
        "\n",
        "> **TODO:** Escriba el código para crear lotes en la función de abajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "MMyKyzdadMsF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "\n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    #print(f\"arr.shape: {arr.shape}\")\n",
        "    ## TO_DO: Calcular el número de lotes que se pueden obter del texto. Llamarlo n_batches.\n",
        "    n_batches = len(arr) // (batch_size * seq_length)\n",
        "    #n_batches = arr.shape[1] // (batch_size * seq_length)\n",
        "\n",
        "    ## TO_DO: Conservar una cantidad exacta de caracteres, de tal forma que sólo haya lotes completos.\n",
        "    arr = arr[:n_batches * batch_size * seq_length]\n",
        "\n",
        "    ## TO_DO: Redimensionar arr de tal forma que quede de tamaño batch_size*(lo que quede)\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    ## Iterar sobre los lotes, usando una ventana de tamaño seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:,n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAVVauLxdMsG"
      },
      "source": [
        "### Compruebe su implementación\n",
        "\n",
        "Crearemos algunos lotes para revisar lo que ocurre. Usaremos batch_size de 8 y tamaño de la secuencia 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y2uE1k_MdMsG"
      },
      "outputs": [],
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v47QnWidMsG",
        "outputId": "76f50e3f-2ede-4bf7-e60c-10e51adbb7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 37  76  17  71 107  72  57  62  76  49]\n",
            " [ 74  77  49  62  72  62  72  76   0  71]\n",
            " [ 49  91   0  72  65  72   0  90  72  18]\n",
            " [  0  60  72   4  62  72  96  34  90   0]\n",
            " [ 72  11  62  38  74  62  91  34   0  49]\n",
            " [  0  91  34  21  72   0  38  72  70   0]\n",
            " [ 72  91   0  96  70  49  62  71  77 107]\n",
            " [ 91  17  72   0  38  72  64  62  33  64]]\n",
            "\n",
            "y\n",
            " [[ 76  17  71 107  72  57  62  76  49   0]\n",
            " [ 77  49  62  72  62  72  76   0  71  91]\n",
            " [ 91   0  72  65  72   0  90  72  18   0]\n",
            " [ 60  72   4  62  72  96  34  90   0  49]\n",
            " [ 11  62  38  74  62  91  34   0  49  49]\n",
            " [ 91  34  21  72   0  38  72  70   0  49]\n",
            " [ 91   0  96  70  49  62  71  77 107  72]\n",
            " [ 17  72   0  38  72  64  62  33  64  62]]\n"
          ]
        }
      ],
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYwV5weOdMsG"
      },
      "source": [
        "Si se implementó `get_batches` correctamente, la salida debería verse similar a esta:\n",
        "```\n",
        "x\n",
        " [[25  8 60 11 45 27 28 73  1  2]\n",
        " [17  7 20 73 45  8 60 45 73 60]\n",
        " [27 20 80 73  7 28 73 60 73 65]\n",
        " [17 73 45  8 27 73 66  8 46 27]\n",
        " [73 17 60 12 73  8 27 28 73 45]\n",
        " [66 64 17 17 46  7 20 73 60 20]\n",
        " [73 76 20 20 60 73  8 60 80 73]\n",
        " [47 35 43  7 20 17 24 50 37 73]]\n",
        "\n",
        "y\n",
        " [[ 8 60 11 45 27 28 73  1  2  2]\n",
        " [ 7 20 73 45  8 60 45 73 60 45]\n",
        " [20 80 73  7 28 73 60 73 65  7]\n",
        " [73 45  8 27 73 66  8 46 27 65]\n",
        " [17 60 12 73  8 27 28 73 45 27]\n",
        " [64 17 17 46  7 20 73 60 20 80]\n",
        " [76 20 20 60 73  8 60 80 73 17]\n",
        " [35 43  7 20 17 24 50 37 73 36]]\n",
        " ```\n",
        "Los números pueden ser diferentes. Asegúrese de que los valores en y están desfasados en 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hu7je8jdMsG"
      },
      "source": [
        "---\n",
        "## Definir la red con Pytorch\n",
        "\n",
        "A continuación definiremos la red. Usamos Pytorch para definir la arquitectura de la red. Comenzamos definiendo el forward pass. Se provee también un método para predecir caracteres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOhMP1uFdMsH"
      },
      "source": [
        "### Esctructura del modelo\n",
        "\n",
        "En `__init__` la estructura sugerida es como sigue:\n",
        "* Crear y guardar los diccionarios requeridos (diccionarios para codificar caracteres)\n",
        "* Definir una capa LSTM que tome como parámetros: un tamaño de entrada (el número de caracteres), el tamaño de la capa oculta `n_hidden`, el número de capas `n_layers`, una probabilidad para Dropout `drop_prob`.\n",
        "* Definir una capa Dropout con `drop_prob`\n",
        "* Definir una capa lineal con parámetros: tamaño de entrada `n_hidden` y tamaño de salida (el número de caracteres)\n",
        "\n",
        "Note que algunos parámetros se han nombrado y están dados en la función `__init__`, y los usamos y guardamos con una línea tipo `self.drop_prob = drop_prob`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrJx90sdMsH"
      },
      "source": [
        "---\n",
        "### LSTM Inputs/Outputs\n",
        "\n",
        "Se puede crear una capa [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) como sigue\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers,\n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "en donde `input_size` es el número de características esperadas en la entrada en cada momento de tiempo, `n_hidden` es el tamaño de las capas ocultas y `n_layers` es el número de capas ocultas. Y podemos añadir un parámetro de Dropout con la probabilidad específica.\n",
        "\n",
        "Necesitamos también inicializar los estados iniciales como ceros. (Eso está hecho ya)\n",
        "\n",
        "\n",
        "```python\n",
        "self.init_hidden()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu2GQa1HdMsH",
        "outputId": "171d7474-4b61-42f1-bae8-89dfc0ee930e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "cArNk7OndMsH"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "\n",
        "        ## TO_DO: definir la capa LSTM.\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, batch_first=True, dropout=drop_prob)\n",
        "\n",
        "        ## TO_DO: definir una capa de Dropout\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "\n",
        "        ## TO_DO: Definir una capa final, la cual es fully-connected, cuya entrada es de tamaño igual al tamaño de las capas ocultas,\n",
        "        # y la salida es en este caso de mismo tamaño de la entrada (un vector con una entrada para cada caracter)\n",
        "\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network.\n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # get RNN outputs\n",
        "        r_out, hidden = self.lstm(x, hidden)\n",
        "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
        "        r_out = r_out.reshape(-1, self.n_hidden)\n",
        "\n",
        "        r_out=self.dropout(r_out)\n",
        "        # get final output\n",
        "        output = self.fc(r_out)\n",
        "\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "\n",
        "        return hidden\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Jr9z_4dMsH"
      },
      "source": [
        "## Entrenamiento de la red\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "WiwadkcCdMsH"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    '''\n",
        "    net.train()\n",
        "\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "\n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "\n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxSQyr3jdMsI"
      },
      "source": [
        "## Inicializamos el modelo\n",
        "\n",
        "Ahora podemos entrenar la red. Primero establecemos los hiperparámetros. Luego definimos las dimensiones de los lotes (batches) e iniciamos el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzNFoGkBdMsI",
        "outputId": "b8551efb-25b1-4d4a-df2d-5e14108a04a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(111, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=111, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "## TO_DO: Establecer los hiperparámetros para su modelo\n",
        "# define and print the net\n",
        "n_hidden=256*2\n",
        "n_layers=3\n",
        "\n",
        "## Definir e imprimir net\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mVJHr5vH2fhK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SHvyzMfdMsJ"
      },
      "source": [
        "## Definir hiperparámetros de entrenamiento y entrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Q0KqhmdMsJ",
        "outputId": "c5e27378-8f07-4634-8469-f8674d29deee",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/100... Step: 10... Loss: 3.1995... Val Loss: 3.2910\n",
            "Epoch: 1/100... Step: 20... Loss: 3.1433... Val Loss: 3.2694\n",
            "Epoch: 1/100... Step: 30... Loss: 3.0943... Val Loss: 3.2510\n",
            "Epoch: 1/100... Step: 40... Loss: 3.1229... Val Loss: 3.2493\n",
            "Epoch: 2/100... Step: 50... Loss: 3.1223... Val Loss: 3.2506\n",
            "Epoch: 2/100... Step: 60... Loss: 3.1052... Val Loss: 3.2488\n",
            "Epoch: 2/100... Step: 70... Loss: 3.1143... Val Loss: 3.2530\n",
            "Epoch: 2/100... Step: 80... Loss: 3.1116... Val Loss: 3.2599\n",
            "Epoch: 2/100... Step: 90... Loss: 3.0703... Val Loss: 3.2577\n",
            "Epoch: 3/100... Step: 100... Loss: 3.0411... Val Loss: 3.2502\n",
            "Epoch: 3/100... Step: 110... Loss: 3.0939... Val Loss: 3.2429\n",
            "Epoch: 3/100... Step: 120... Loss: 3.0458... Val Loss: 3.2602\n",
            "Epoch: 3/100... Step: 130... Loss: 2.9929... Val Loss: 3.1762\n",
            "Epoch: 3/100... Step: 140... Loss: 2.7993... Val Loss: 3.1181\n",
            "Epoch: 4/100... Step: 150... Loss: 2.7456... Val Loss: 3.1004\n",
            "Epoch: 4/100... Step: 160... Loss: 2.6921... Val Loss: 3.1061\n",
            "Epoch: 4/100... Step: 170... Loss: 2.6092... Val Loss: 3.0720\n",
            "Epoch: 4/100... Step: 180... Loss: 2.5852... Val Loss: 3.0650\n",
            "Epoch: 4/100... Step: 190... Loss: 2.5047... Val Loss: 3.0301\n",
            "Epoch: 5/100... Step: 200... Loss: 2.4474... Val Loss: 2.9752\n",
            "Epoch: 5/100... Step: 210... Loss: 2.4345... Val Loss: 2.9735\n",
            "Epoch: 5/100... Step: 220... Loss: 2.4111... Val Loss: 2.9215\n",
            "Epoch: 5/100... Step: 230... Loss: 2.3657... Val Loss: 2.9084\n",
            "Epoch: 5/100... Step: 240... Loss: 2.2923... Val Loss: 2.9213\n",
            "Epoch: 6/100... Step: 250... Loss: 2.2752... Val Loss: 2.8730\n",
            "Epoch: 6/100... Step: 260... Loss: 2.2920... Val Loss: 2.8500\n",
            "Epoch: 6/100... Step: 270... Loss: 2.2625... Val Loss: 2.8301\n",
            "Epoch: 6/100... Step: 280... Loss: 2.2589... Val Loss: 2.8242\n",
            "Epoch: 6/100... Step: 290... Loss: 2.2201... Val Loss: 2.8273\n",
            "Epoch: 7/100... Step: 300... Loss: 2.2359... Val Loss: 2.8125\n",
            "Epoch: 7/100... Step: 310... Loss: 2.2124... Val Loss: 2.7976\n",
            "Epoch: 7/100... Step: 320... Loss: 2.1969... Val Loss: 2.7790\n",
            "Epoch: 7/100... Step: 330... Loss: 2.1531... Val Loss: 2.7712\n",
            "Epoch: 7/100... Step: 340... Loss: 2.1857... Val Loss: 2.7786\n",
            "Epoch: 8/100... Step: 350... Loss: 2.1563... Val Loss: 2.7586\n",
            "Epoch: 8/100... Step: 360... Loss: 2.1481... Val Loss: 2.7485\n",
            "Epoch: 8/100... Step: 370... Loss: 2.1810... Val Loss: 2.7392\n",
            "Epoch: 8/100... Step: 380... Loss: 2.1224... Val Loss: 2.7426\n",
            "Epoch: 8/100... Step: 390... Loss: 2.1078... Val Loss: 2.7483\n",
            "Epoch: 9/100... Step: 400... Loss: 2.1252... Val Loss: 2.7302\n",
            "Epoch: 9/100... Step: 410... Loss: 2.1021... Val Loss: 2.7253\n",
            "Epoch: 9/100... Step: 420... Loss: 2.0569... Val Loss: 2.7190\n",
            "Epoch: 9/100... Step: 430... Loss: 2.0999... Val Loss: 2.7135\n",
            "Epoch: 9/100... Step: 440... Loss: 2.0850... Val Loss: 2.7144\n",
            "Epoch: 10/100... Step: 450... Loss: 2.0828... Val Loss: 2.6928\n",
            "Epoch: 10/100... Step: 460... Loss: 2.0298... Val Loss: 2.6857\n",
            "Epoch: 10/100... Step: 470... Loss: 2.0411... Val Loss: 2.6934\n",
            "Epoch: 10/100... Step: 480... Loss: 2.0332... Val Loss: 2.6659\n",
            "Epoch: 10/100... Step: 490... Loss: 2.0522... Val Loss: 2.6718\n",
            "Epoch: 11/100... Step: 500... Loss: 2.0102... Val Loss: 2.6474\n",
            "Epoch: 11/100... Step: 510... Loss: 2.0016... Val Loss: 2.6536\n",
            "Epoch: 11/100... Step: 520... Loss: 1.9587... Val Loss: 2.6428\n",
            "Epoch: 11/100... Step: 530... Loss: 1.9832... Val Loss: 2.6354\n",
            "Epoch: 12/100... Step: 540... Loss: 2.0186... Val Loss: 2.6308\n",
            "Epoch: 12/100... Step: 550... Loss: 1.9807... Val Loss: 2.6214\n",
            "Epoch: 12/100... Step: 560... Loss: 1.9628... Val Loss: 2.6153\n",
            "Epoch: 12/100... Step: 570... Loss: 1.9550... Val Loss: 2.6132\n",
            "Epoch: 12/100... Step: 580... Loss: 1.9413... Val Loss: 2.6091\n",
            "Epoch: 13/100... Step: 590... Loss: 1.9152... Val Loss: 2.5965\n",
            "Epoch: 13/100... Step: 600... Loss: 1.9384... Val Loss: 2.5874\n",
            "Epoch: 13/100... Step: 610... Loss: 1.8923... Val Loss: 2.5995\n",
            "Epoch: 13/100... Step: 620... Loss: 1.8979... Val Loss: 2.5826\n",
            "Epoch: 13/100... Step: 630... Loss: 1.8898... Val Loss: 2.5993\n",
            "Epoch: 14/100... Step: 640... Loss: 1.8907... Val Loss: 2.5637\n",
            "Epoch: 14/100... Step: 650... Loss: 1.8789... Val Loss: 2.5559\n",
            "Epoch: 14/100... Step: 660... Loss: 1.8633... Val Loss: 2.5673\n",
            "Epoch: 14/100... Step: 670... Loss: 1.9031... Val Loss: 2.5351\n",
            "Epoch: 14/100... Step: 680... Loss: 1.8635... Val Loss: 2.5468\n",
            "Epoch: 15/100... Step: 690... Loss: 1.8551... Val Loss: 2.5395\n",
            "Epoch: 15/100... Step: 700... Loss: 1.8763... Val Loss: 2.5379\n",
            "Epoch: 15/100... Step: 710... Loss: 1.8555... Val Loss: 2.5520\n",
            "Epoch: 15/100... Step: 720... Loss: 1.8445... Val Loss: 2.5320\n",
            "Epoch: 15/100... Step: 730... Loss: 1.8040... Val Loss: 2.5244\n",
            "Epoch: 16/100... Step: 740... Loss: 1.7975... Val Loss: 2.5318\n",
            "Epoch: 16/100... Step: 750... Loss: 1.8106... Val Loss: 2.5118\n",
            "Epoch: 16/100... Step: 760... Loss: 1.8129... Val Loss: 2.5277\n",
            "Epoch: 16/100... Step: 770... Loss: 1.8351... Val Loss: 2.5092\n",
            "Epoch: 16/100... Step: 780... Loss: 1.7728... Val Loss: 2.5248\n",
            "Epoch: 17/100... Step: 790... Loss: 1.8043... Val Loss: 2.4999\n",
            "Epoch: 17/100... Step: 800... Loss: 1.7818... Val Loss: 2.5124\n",
            "Epoch: 17/100... Step: 810... Loss: 1.7696... Val Loss: 2.5244\n",
            "Epoch: 17/100... Step: 820... Loss: 1.7335... Val Loss: 2.5134\n",
            "Epoch: 17/100... Step: 830... Loss: 1.7732... Val Loss: 2.5097\n",
            "Epoch: 18/100... Step: 840... Loss: 1.7341... Val Loss: 2.4954\n",
            "Epoch: 18/100... Step: 850... Loss: 1.7389... Val Loss: 2.4821\n",
            "Epoch: 18/100... Step: 860... Loss: 1.7624... Val Loss: 2.5104\n",
            "Epoch: 18/100... Step: 870... Loss: 1.7251... Val Loss: 2.5057\n",
            "Epoch: 18/100... Step: 880... Loss: 1.7362... Val Loss: 2.4974\n",
            "Epoch: 19/100... Step: 890... Loss: 1.7415... Val Loss: 2.4767\n",
            "Epoch: 19/100... Step: 900... Loss: 1.7338... Val Loss: 2.4902\n",
            "Epoch: 19/100... Step: 910... Loss: 1.6836... Val Loss: 2.5055\n",
            "Epoch: 19/100... Step: 920... Loss: 1.7438... Val Loss: 2.4972\n",
            "Epoch: 19/100... Step: 930... Loss: 1.7265... Val Loss: 2.4820\n",
            "Epoch: 20/100... Step: 940... Loss: 1.7196... Val Loss: 2.4852\n",
            "Epoch: 20/100... Step: 950... Loss: 1.6598... Val Loss: 2.4829\n",
            "Epoch: 20/100... Step: 960... Loss: 1.7015... Val Loss: 2.4878\n",
            "Epoch: 20/100... Step: 970... Loss: 1.6896... Val Loss: 2.4824\n",
            "Epoch: 20/100... Step: 980... Loss: 1.7035... Val Loss: 2.4902\n",
            "Epoch: 21/100... Step: 990... Loss: 1.6724... Val Loss: 2.4659\n",
            "Epoch: 21/100... Step: 1000... Loss: 1.6655... Val Loss: 2.4805\n",
            "Epoch: 21/100... Step: 1010... Loss: 1.6086... Val Loss: 2.4883\n",
            "Epoch: 21/100... Step: 1020... Loss: 1.6658... Val Loss: 2.4731\n",
            "Epoch: 22/100... Step: 1030... Loss: 1.7048... Val Loss: 2.4773\n",
            "Epoch: 22/100... Step: 1040... Loss: 1.6616... Val Loss: 2.4761\n",
            "Epoch: 22/100... Step: 1050... Loss: 1.6718... Val Loss: 2.4784\n",
            "Epoch: 22/100... Step: 1060... Loss: 1.6379... Val Loss: 2.4926\n",
            "Epoch: 22/100... Step: 1070... Loss: 1.6662... Val Loss: 2.4714\n",
            "Epoch: 23/100... Step: 1080... Loss: 1.6363... Val Loss: 2.4862\n",
            "Epoch: 23/100... Step: 1090... Loss: 1.6437... Val Loss: 2.4695\n",
            "Epoch: 23/100... Step: 1100... Loss: 1.6060... Val Loss: 2.4908\n",
            "Epoch: 23/100... Step: 1110... Loss: 1.6072... Val Loss: 2.4962\n",
            "Epoch: 23/100... Step: 1120... Loss: 1.6087... Val Loss: 2.4675\n",
            "Epoch: 24/100... Step: 1130... Loss: 1.6250... Val Loss: 2.4680\n",
            "Epoch: 24/100... Step: 1140... Loss: 1.6077... Val Loss: 2.4721\n",
            "Epoch: 24/100... Step: 1150... Loss: 1.5921... Val Loss: 2.4720\n",
            "Epoch: 24/100... Step: 1160... Loss: 1.6248... Val Loss: 2.4825\n",
            "Epoch: 24/100... Step: 1170... Loss: 1.5949... Val Loss: 2.4689\n",
            "Epoch: 25/100... Step: 1180... Loss: 1.5965... Val Loss: 2.4722\n",
            "Epoch: 25/100... Step: 1190... Loss: 1.6112... Val Loss: 2.4562\n",
            "Epoch: 25/100... Step: 1200... Loss: 1.5999... Val Loss: 2.4900\n",
            "Epoch: 25/100... Step: 1210... Loss: 1.5922... Val Loss: 2.4791\n",
            "Epoch: 25/100... Step: 1220... Loss: 1.5591... Val Loss: 2.4749\n",
            "Epoch: 26/100... Step: 1230... Loss: 1.5618... Val Loss: 2.4736\n",
            "Epoch: 26/100... Step: 1240... Loss: 1.5633... Val Loss: 2.4677\n",
            "Epoch: 26/100... Step: 1250... Loss: 1.5623... Val Loss: 2.4812\n",
            "Epoch: 26/100... Step: 1260... Loss: 1.5862... Val Loss: 2.4702\n",
            "Epoch: 26/100... Step: 1270... Loss: 1.5406... Val Loss: 2.4742\n",
            "Epoch: 27/100... Step: 1280... Loss: 1.5853... Val Loss: 2.4776\n",
            "Epoch: 27/100... Step: 1290... Loss: 1.5673... Val Loss: 2.4572\n",
            "Epoch: 27/100... Step: 1300... Loss: 1.5459... Val Loss: 2.4847\n",
            "Epoch: 27/100... Step: 1310... Loss: 1.5107... Val Loss: 2.4607\n",
            "Epoch: 27/100... Step: 1320... Loss: 1.5583... Val Loss: 2.4615\n",
            "Epoch: 28/100... Step: 1330... Loss: 1.5153... Val Loss: 2.4828\n",
            "Epoch: 28/100... Step: 1340... Loss: 1.5187... Val Loss: 2.4687\n",
            "Epoch: 28/100... Step: 1350... Loss: 1.5387... Val Loss: 2.4600\n",
            "Epoch: 28/100... Step: 1360... Loss: 1.5334... Val Loss: 2.4668\n",
            "Epoch: 28/100... Step: 1370... Loss: 1.5432... Val Loss: 2.4667\n",
            "Epoch: 29/100... Step: 1380... Loss: 1.5410... Val Loss: 2.4726\n",
            "Epoch: 29/100... Step: 1390... Loss: 1.5289... Val Loss: 2.4965\n",
            "Epoch: 29/100... Step: 1400... Loss: 1.5087... Val Loss: 2.4837\n",
            "Epoch: 29/100... Step: 1410... Loss: 1.5443... Val Loss: 2.4677\n",
            "Epoch: 29/100... Step: 1420... Loss: 1.5258... Val Loss: 2.4624\n",
            "Epoch: 30/100... Step: 1430... Loss: 1.5141... Val Loss: 2.4911\n",
            "Epoch: 30/100... Step: 1440... Loss: 1.4678... Val Loss: 2.4939\n",
            "Epoch: 30/100... Step: 1450... Loss: 1.5024... Val Loss: 2.4825\n",
            "Epoch: 30/100... Step: 1460... Loss: 1.5036... Val Loss: 2.4922\n",
            "Epoch: 30/100... Step: 1470... Loss: 1.5272... Val Loss: 2.4868\n",
            "Epoch: 31/100... Step: 1480... Loss: 1.4797... Val Loss: 2.4771\n",
            "Epoch: 31/100... Step: 1490... Loss: 1.4978... Val Loss: 2.4856\n",
            "Epoch: 31/100... Step: 1500... Loss: 1.4297... Val Loss: 2.4795\n",
            "Epoch: 31/100... Step: 1510... Loss: 1.4988... Val Loss: 2.4681\n",
            "Epoch: 32/100... Step: 1520... Loss: 1.5252... Val Loss: 2.4816\n",
            "Epoch: 32/100... Step: 1530... Loss: 1.4855... Val Loss: 2.4740\n",
            "Epoch: 32/100... Step: 1540... Loss: 1.4960... Val Loss: 2.4864\n",
            "Epoch: 32/100... Step: 1550... Loss: 1.4759... Val Loss: 2.4885\n",
            "Epoch: 32/100... Step: 1560... Loss: 1.5181... Val Loss: 2.4792\n",
            "Epoch: 33/100... Step: 1570... Loss: 1.4593... Val Loss: 2.4820\n",
            "Epoch: 33/100... Step: 1580... Loss: 1.4975... Val Loss: 2.4590\n",
            "Epoch: 33/100... Step: 1590... Loss: 1.4657... Val Loss: 2.4842\n",
            "Epoch: 33/100... Step: 1600... Loss: 1.4496... Val Loss: 2.5071\n",
            "Epoch: 33/100... Step: 1610... Loss: 1.4736... Val Loss: 2.4953\n",
            "Epoch: 34/100... Step: 1620... Loss: 1.4764... Val Loss: 2.4958\n",
            "Epoch: 34/100... Step: 1630... Loss: 1.4731... Val Loss: 2.4746\n",
            "Epoch: 34/100... Step: 1640... Loss: 1.4408... Val Loss: 2.4874\n",
            "Epoch: 34/100... Step: 1650... Loss: 1.4726... Val Loss: 2.4910\n",
            "Epoch: 34/100... Step: 1660... Loss: 1.4461... Val Loss: 2.5025\n",
            "Epoch: 35/100... Step: 1670... Loss: 1.4458... Val Loss: 2.5052\n",
            "Epoch: 35/100... Step: 1680... Loss: 1.4736... Val Loss: 2.4798\n",
            "Epoch: 35/100... Step: 1690... Loss: 1.4553... Val Loss: 2.5083\n",
            "Epoch: 35/100... Step: 1700... Loss: 1.4524... Val Loss: 2.5033\n",
            "Epoch: 35/100... Step: 1710... Loss: 1.4351... Val Loss: 2.5119\n",
            "Epoch: 36/100... Step: 1720... Loss: 1.4208... Val Loss: 2.5394\n",
            "Epoch: 36/100... Step: 1730... Loss: 1.4153... Val Loss: 2.5043\n",
            "Epoch: 36/100... Step: 1740... Loss: 1.4321... Val Loss: 2.5039\n",
            "Epoch: 36/100... Step: 1750... Loss: 1.4673... Val Loss: 2.4883\n",
            "Epoch: 36/100... Step: 1760... Loss: 1.3907... Val Loss: 2.4971\n",
            "Epoch: 37/100... Step: 1770... Loss: 1.4442... Val Loss: 2.5090\n",
            "Epoch: 37/100... Step: 1780... Loss: 1.4260... Val Loss: 2.4885\n",
            "Epoch: 37/100... Step: 1790... Loss: 1.4084... Val Loss: 2.5005\n",
            "Epoch: 37/100... Step: 1800... Loss: 1.3788... Val Loss: 2.4987\n",
            "Epoch: 37/100... Step: 1810... Loss: 1.4309... Val Loss: 2.4909\n",
            "Epoch: 38/100... Step: 1820... Loss: 1.3933... Val Loss: 2.4973\n",
            "Epoch: 38/100... Step: 1830... Loss: 1.3992... Val Loss: 2.4753\n",
            "Epoch: 38/100... Step: 1840... Loss: 1.4142... Val Loss: 2.4947\n",
            "Epoch: 38/100... Step: 1850... Loss: 1.4033... Val Loss: 2.4973\n",
            "Epoch: 38/100... Step: 1860... Loss: 1.4223... Val Loss: 2.5144\n",
            "Epoch: 39/100... Step: 1870... Loss: 1.4061... Val Loss: 2.5082\n",
            "Epoch: 39/100... Step: 1880... Loss: 1.3985... Val Loss: 2.4992\n",
            "Epoch: 39/100... Step: 1890... Loss: 1.3771... Val Loss: 2.5303\n",
            "Epoch: 39/100... Step: 1900... Loss: 1.4195... Val Loss: 2.5094\n",
            "Epoch: 39/100... Step: 1910... Loss: 1.3993... Val Loss: 2.4884\n",
            "Epoch: 40/100... Step: 1920... Loss: 1.3843... Val Loss: 2.4912\n",
            "Epoch: 40/100... Step: 1930... Loss: 1.3379... Val Loss: 2.5197\n",
            "Epoch: 40/100... Step: 1940... Loss: 1.3878... Val Loss: 2.5514\n",
            "Epoch: 40/100... Step: 1950... Loss: 1.3962... Val Loss: 2.5196\n",
            "Epoch: 40/100... Step: 1960... Loss: 1.4114... Val Loss: 2.5090\n",
            "Epoch: 41/100... Step: 1970... Loss: 1.3719... Val Loss: 2.5194\n",
            "Epoch: 41/100... Step: 1980... Loss: 1.3775... Val Loss: 2.5385\n",
            "Epoch: 41/100... Step: 1990... Loss: 1.3242... Val Loss: 2.5480\n",
            "Epoch: 41/100... Step: 2000... Loss: 1.3875... Val Loss: 2.5469\n",
            "Epoch: 42/100... Step: 2010... Loss: 1.4127... Val Loss: 2.5297\n",
            "Epoch: 42/100... Step: 2020... Loss: 1.3655... Val Loss: 2.5458\n",
            "Epoch: 42/100... Step: 2030... Loss: 1.3753... Val Loss: 2.5692\n",
            "Epoch: 42/100... Step: 2040... Loss: 1.3708... Val Loss: 2.5725\n",
            "Epoch: 42/100... Step: 2050... Loss: 1.3971... Val Loss: 2.5689\n",
            "Epoch: 43/100... Step: 2060... Loss: 1.3752... Val Loss: 2.5523\n",
            "Epoch: 43/100... Step: 2070... Loss: 1.3920... Val Loss: 2.5590\n",
            "Epoch: 43/100... Step: 2080... Loss: 1.3607... Val Loss: 2.5932\n",
            "Epoch: 43/100... Step: 2090... Loss: 1.3488... Val Loss: 2.5802\n",
            "Epoch: 43/100... Step: 2100... Loss: 1.3650... Val Loss: 2.5840\n",
            "Epoch: 44/100... Step: 2110... Loss: 1.3695... Val Loss: 2.5825\n",
            "Epoch: 44/100... Step: 2120... Loss: 1.3614... Val Loss: 2.5622\n",
            "Epoch: 44/100... Step: 2130... Loss: 1.3533... Val Loss: 2.5990\n",
            "Epoch: 44/100... Step: 2140... Loss: 1.3704... Val Loss: 2.5878\n",
            "Epoch: 44/100... Step: 2150... Loss: 1.3356... Val Loss: 2.5740\n",
            "Epoch: 45/100... Step: 2160... Loss: 1.3534... Val Loss: 2.5749\n",
            "Epoch: 45/100... Step: 2170... Loss: 1.3752... Val Loss: 2.5509\n",
            "Epoch: 45/100... Step: 2180... Loss: 1.3603... Val Loss: 2.6128\n",
            "Epoch: 45/100... Step: 2190... Loss: 1.3503... Val Loss: 2.5977\n",
            "Epoch: 45/100... Step: 2200... Loss: 1.3275... Val Loss: 2.6099\n",
            "Epoch: 46/100... Step: 2210... Loss: 1.3281... Val Loss: 2.6396\n",
            "Epoch: 46/100... Step: 2220... Loss: 1.3269... Val Loss: 2.6043\n",
            "Epoch: 46/100... Step: 2230... Loss: 1.3507... Val Loss: 2.6260\n",
            "Epoch: 46/100... Step: 2240... Loss: 1.3665... Val Loss: 2.6308\n",
            "Epoch: 46/100... Step: 2250... Loss: 1.3054... Val Loss: 2.6096\n",
            "Epoch: 47/100... Step: 2260... Loss: 1.3506... Val Loss: 2.5962\n",
            "Epoch: 47/100... Step: 2270... Loss: 1.3327... Val Loss: 2.5734\n",
            "Epoch: 47/100... Step: 2280... Loss: 1.3225... Val Loss: 2.6214\n",
            "Epoch: 47/100... Step: 2290... Loss: 1.2986... Val Loss: 2.5910\n",
            "Epoch: 47/100... Step: 2300... Loss: 1.3333... Val Loss: 2.6043\n",
            "Epoch: 48/100... Step: 2310... Loss: 1.2970... Val Loss: 2.6284\n",
            "Epoch: 48/100... Step: 2320... Loss: 1.3058... Val Loss: 2.5948\n",
            "Epoch: 48/100... Step: 2330... Loss: 1.3263... Val Loss: 2.5850\n",
            "Epoch: 48/100... Step: 2340... Loss: 1.3274... Val Loss: 2.5832\n",
            "Epoch: 48/100... Step: 2350... Loss: 1.3307... Val Loss: 2.5941\n",
            "Epoch: 49/100... Step: 2360... Loss: 1.3084... Val Loss: 2.6300\n",
            "Epoch: 49/100... Step: 2370... Loss: 1.3080... Val Loss: 2.6029\n",
            "Epoch: 49/100... Step: 2380... Loss: 1.3009... Val Loss: 2.6296\n",
            "Epoch: 49/100... Step: 2390... Loss: 1.3262... Val Loss: 2.6253\n",
            "Epoch: 49/100... Step: 2400... Loss: 1.3220... Val Loss: 2.6126\n",
            "Epoch: 50/100... Step: 2410... Loss: 1.3078... Val Loss: 2.6219\n",
            "Epoch: 50/100... Step: 2420... Loss: 1.2634... Val Loss: 2.6419\n",
            "Epoch: 50/100... Step: 2430... Loss: 1.3100... Val Loss: 2.6347\n",
            "Epoch: 50/100... Step: 2440... Loss: 1.3079... Val Loss: 2.6196\n",
            "Epoch: 50/100... Step: 2450... Loss: 1.3218... Val Loss: 2.6240\n",
            "Epoch: 51/100... Step: 2460... Loss: 1.2894... Val Loss: 2.6103\n",
            "Epoch: 51/100... Step: 2470... Loss: 1.3032... Val Loss: 2.6216\n",
            "Epoch: 51/100... Step: 2480... Loss: 1.2506... Val Loss: 2.6074\n",
            "Epoch: 51/100... Step: 2490... Loss: 1.3109... Val Loss: 2.5995\n",
            "Epoch: 52/100... Step: 2500... Loss: 1.3343... Val Loss: 2.5868\n",
            "Epoch: 52/100... Step: 2510... Loss: 1.2902... Val Loss: 2.5963\n",
            "Epoch: 52/100... Step: 2520... Loss: 1.3073... Val Loss: 2.6329\n",
            "Epoch: 52/100... Step: 2530... Loss: 1.3033... Val Loss: 2.6500\n",
            "Epoch: 52/100... Step: 2540... Loss: 1.3232... Val Loss: 2.6271\n",
            "Epoch: 53/100... Step: 2550... Loss: 1.2835... Val Loss: 2.6037\n",
            "Epoch: 53/100... Step: 2560... Loss: 1.3012... Val Loss: 2.5928\n",
            "Epoch: 53/100... Step: 2570... Loss: 1.2844... Val Loss: 2.6233\n",
            "Epoch: 53/100... Step: 2580... Loss: 1.2729... Val Loss: 2.6248\n",
            "Epoch: 53/100... Step: 2590... Loss: 1.2848... Val Loss: 2.6435\n",
            "Epoch: 54/100... Step: 2600... Loss: 1.2923... Val Loss: 2.6136\n",
            "Epoch: 54/100... Step: 2610... Loss: 1.2829... Val Loss: 2.5936\n",
            "Epoch: 54/100... Step: 2620... Loss: 1.2577... Val Loss: 2.6163\n",
            "Epoch: 54/100... Step: 2630... Loss: 1.2937... Val Loss: 2.6157\n",
            "Epoch: 54/100... Step: 2640... Loss: 1.2752... Val Loss: 2.6492\n",
            "Epoch: 55/100... Step: 2650... Loss: 1.2821... Val Loss: 2.6228\n",
            "Epoch: 55/100... Step: 2660... Loss: 1.2854... Val Loss: 2.5673\n",
            "Epoch: 55/100... Step: 2670... Loss: 1.2898... Val Loss: 2.6349\n",
            "Epoch: 55/100... Step: 2680... Loss: 1.2742... Val Loss: 2.6365\n",
            "Epoch: 55/100... Step: 2690... Loss: 1.2621... Val Loss: 2.6707\n",
            "Epoch: 56/100... Step: 2700... Loss: 1.2591... Val Loss: 2.6413\n",
            "Epoch: 56/100... Step: 2710... Loss: 1.2441... Val Loss: 2.6058\n",
            "Epoch: 56/100... Step: 2720... Loss: 1.2724... Val Loss: 2.6307\n",
            "Epoch: 56/100... Step: 2730... Loss: 1.2918... Val Loss: 2.6550\n",
            "Epoch: 56/100... Step: 2740... Loss: 1.2458... Val Loss: 2.7154\n",
            "Epoch: 57/100... Step: 2750... Loss: 1.2671... Val Loss: 2.6457\n",
            "Epoch: 57/100... Step: 2760... Loss: 1.2684... Val Loss: 2.6217\n",
            "Epoch: 57/100... Step: 2770... Loss: 1.2427... Val Loss: 2.6475\n",
            "Epoch: 57/100... Step: 2780... Loss: 1.2286... Val Loss: 2.6535\n",
            "Epoch: 57/100... Step: 2790... Loss: 1.2740... Val Loss: 2.6905\n",
            "Epoch: 58/100... Step: 2800... Loss: 1.2279... Val Loss: 2.6591\n",
            "Epoch: 58/100... Step: 2810... Loss: 1.2387... Val Loss: 2.6467\n",
            "Epoch: 58/100... Step: 2820... Loss: 1.2483... Val Loss: 2.6590\n",
            "Epoch: 58/100... Step: 2830... Loss: 1.2499... Val Loss: 2.6462\n",
            "Epoch: 58/100... Step: 2840... Loss: 1.2622... Val Loss: 2.6783\n",
            "Epoch: 59/100... Step: 2850... Loss: 1.2424... Val Loss: 2.6703\n",
            "Epoch: 59/100... Step: 2860... Loss: 1.2539... Val Loss: 2.6527\n",
            "Epoch: 59/100... Step: 2870... Loss: 1.2427... Val Loss: 2.6749\n",
            "Epoch: 59/100... Step: 2880... Loss: 1.2505... Val Loss: 2.6628\n",
            "Epoch: 59/100... Step: 2890... Loss: 1.2445... Val Loss: 2.7081\n",
            "Epoch: 60/100... Step: 2900... Loss: 1.2298... Val Loss: 2.6637\n",
            "Epoch: 60/100... Step: 2910... Loss: 1.2025... Val Loss: 2.6928\n",
            "Epoch: 60/100... Step: 2920... Loss: 1.2539... Val Loss: 2.6970\n",
            "Epoch: 60/100... Step: 2930... Loss: 1.2353... Val Loss: 2.6764\n",
            "Epoch: 60/100... Step: 2940... Loss: 1.2611... Val Loss: 2.7005\n",
            "Epoch: 61/100... Step: 2950... Loss: 1.2277... Val Loss: 2.6806\n",
            "Epoch: 61/100... Step: 2960... Loss: 1.2256... Val Loss: 2.7229\n",
            "Epoch: 61/100... Step: 2970... Loss: 1.2032... Val Loss: 2.7190\n",
            "Epoch: 61/100... Step: 2980... Loss: 1.2479... Val Loss: 2.7042\n",
            "Epoch: 62/100... Step: 2990... Loss: 1.2562... Val Loss: 2.7397\n",
            "Epoch: 62/100... Step: 3000... Loss: 1.2323... Val Loss: 2.6886\n",
            "Epoch: 62/100... Step: 3010... Loss: 1.2343... Val Loss: 2.7591\n",
            "Epoch: 62/100... Step: 3020... Loss: 1.2373... Val Loss: 2.7642\n",
            "Epoch: 62/100... Step: 3030... Loss: 1.2602... Val Loss: 2.7336\n",
            "Epoch: 63/100... Step: 3040... Loss: 1.2252... Val Loss: 2.7290\n",
            "Epoch: 63/100... Step: 3050... Loss: 1.2460... Val Loss: 2.6979\n",
            "Epoch: 63/100... Step: 3060... Loss: 1.2093... Val Loss: 2.7625\n",
            "Epoch: 63/100... Step: 3070... Loss: 1.2046... Val Loss: 2.7445\n",
            "Epoch: 63/100... Step: 3080... Loss: 1.2171... Val Loss: 2.7307\n",
            "Epoch: 64/100... Step: 3090... Loss: 1.2342... Val Loss: 2.7554\n",
            "Epoch: 64/100... Step: 3100... Loss: 1.2241... Val Loss: 2.7126\n",
            "Epoch: 64/100... Step: 3110... Loss: 1.1902... Val Loss: 2.7740\n",
            "Epoch: 64/100... Step: 3120... Loss: 1.2310... Val Loss: 2.7444\n",
            "Epoch: 64/100... Step: 3130... Loss: 1.2017... Val Loss: 2.7363\n",
            "Epoch: 65/100... Step: 3140... Loss: 1.2103... Val Loss: 2.7669\n",
            "Epoch: 65/100... Step: 3150... Loss: 1.2247... Val Loss: 2.7100\n",
            "Epoch: 65/100... Step: 3160... Loss: 1.2238... Val Loss: 2.7326\n",
            "Epoch: 65/100... Step: 3170... Loss: 1.2151... Val Loss: 2.7848\n",
            "Epoch: 65/100... Step: 3180... Loss: 1.1975... Val Loss: 2.7450\n",
            "Epoch: 66/100... Step: 3190... Loss: 1.1967... Val Loss: 2.7779\n",
            "Epoch: 66/100... Step: 3200... Loss: 1.1897... Val Loss: 2.7300\n",
            "Epoch: 66/100... Step: 3210... Loss: 1.2047... Val Loss: 2.7477\n",
            "Epoch: 66/100... Step: 3220... Loss: 1.2353... Val Loss: 2.8063\n",
            "Epoch: 66/100... Step: 3230... Loss: 1.1750... Val Loss: 2.7949\n",
            "Epoch: 67/100... Step: 3240... Loss: 1.2163... Val Loss: 2.7761\n",
            "Epoch: 67/100... Step: 3250... Loss: 1.2023... Val Loss: 2.7062\n",
            "Epoch: 67/100... Step: 3260... Loss: 1.1811... Val Loss: 2.7957\n",
            "Epoch: 67/100... Step: 3270... Loss: 1.1840... Val Loss: 2.7977\n",
            "Epoch: 67/100... Step: 3280... Loss: 1.2002... Val Loss: 2.8238\n",
            "Epoch: 68/100... Step: 3290... Loss: 1.1773... Val Loss: 2.8201\n",
            "Epoch: 68/100... Step: 3300... Loss: 1.1956... Val Loss: 2.7277\n",
            "Epoch: 68/100... Step: 3310... Loss: 1.1807... Val Loss: 2.7787\n",
            "Epoch: 68/100... Step: 3320... Loss: 1.1864... Val Loss: 2.7659\n",
            "Epoch: 68/100... Step: 3330... Loss: 1.2047... Val Loss: 2.8290\n",
            "Epoch: 69/100... Step: 3340... Loss: 1.1876... Val Loss: 2.8079\n",
            "Epoch: 69/100... Step: 3350... Loss: 1.1941... Val Loss: 2.7857\n",
            "Epoch: 69/100... Step: 3360... Loss: 1.1717... Val Loss: 2.7983\n",
            "Epoch: 69/100... Step: 3370... Loss: 1.2019... Val Loss: 2.7636\n",
            "Epoch: 69/100... Step: 3380... Loss: 1.1894... Val Loss: 2.8405\n",
            "Epoch: 70/100... Step: 3390... Loss: 1.1860... Val Loss: 2.8295\n",
            "Epoch: 70/100... Step: 3400... Loss: 1.1610... Val Loss: 2.7892\n",
            "Epoch: 70/100... Step: 3410... Loss: 1.1900... Val Loss: 2.8020\n",
            "Epoch: 70/100... Step: 3420... Loss: 1.1852... Val Loss: 2.7860\n",
            "Epoch: 70/100... Step: 3430... Loss: 1.2101... Val Loss: 2.8259\n",
            "Epoch: 71/100... Step: 3440... Loss: 1.1794... Val Loss: 2.8292\n",
            "Epoch: 71/100... Step: 3450... Loss: 1.1666... Val Loss: 2.7757\n",
            "Epoch: 71/100... Step: 3460... Loss: 1.1453... Val Loss: 2.8559\n",
            "Epoch: 71/100... Step: 3470... Loss: 1.1907... Val Loss: 2.8058\n",
            "Epoch: 72/100... Step: 3480... Loss: 1.2105... Val Loss: 2.8293\n",
            "Epoch: 72/100... Step: 3490... Loss: 1.1702... Val Loss: 2.8111\n",
            "Epoch: 72/100... Step: 3500... Loss: 1.1779... Val Loss: 2.8332\n",
            "Epoch: 72/100... Step: 3510... Loss: 1.1870... Val Loss: 2.8544\n",
            "Epoch: 72/100... Step: 3520... Loss: 1.2140... Val Loss: 2.7939\n",
            "Epoch: 73/100... Step: 3530... Loss: 1.1862... Val Loss: 2.8310\n",
            "Epoch: 73/100... Step: 3540... Loss: 1.1993... Val Loss: 2.7513\n",
            "Epoch: 73/100... Step: 3550... Loss: 1.1719... Val Loss: 2.8580\n",
            "Epoch: 73/100... Step: 3560... Loss: 1.1766... Val Loss: 2.8794\n",
            "Epoch: 73/100... Step: 3570... Loss: 1.1800... Val Loss: 2.8321\n",
            "Epoch: 74/100... Step: 3580... Loss: 1.1815... Val Loss: 2.8402\n",
            "Epoch: 74/100... Step: 3590... Loss: 1.1838... Val Loss: 2.7979\n",
            "Epoch: 74/100... Step: 3600... Loss: 1.1534... Val Loss: 2.9019\n",
            "Epoch: 74/100... Step: 3610... Loss: 1.1808... Val Loss: 2.9145\n",
            "Epoch: 74/100... Step: 3620... Loss: 1.1776... Val Loss: 2.7880\n",
            "Epoch: 75/100... Step: 3630... Loss: 1.1647... Val Loss: 2.7999\n",
            "Epoch: 75/100... Step: 3640... Loss: 1.1898... Val Loss: 2.7625\n",
            "Epoch: 75/100... Step: 3650... Loss: 1.1915... Val Loss: 2.8838\n",
            "Epoch: 75/100... Step: 3660... Loss: 1.1731... Val Loss: 2.8903\n",
            "Epoch: 75/100... Step: 3670... Loss: 1.1565... Val Loss: 2.7815\n",
            "Epoch: 76/100... Step: 3680... Loss: 1.1457... Val Loss: 2.8353\n",
            "Epoch: 76/100... Step: 3690... Loss: 1.1580... Val Loss: 2.8100\n",
            "Epoch: 76/100... Step: 3700... Loss: 1.1683... Val Loss: 2.9122\n",
            "Epoch: 76/100... Step: 3710... Loss: 1.1978... Val Loss: 2.8876\n",
            "Epoch: 76/100... Step: 3720... Loss: 1.1401... Val Loss: 2.7989\n",
            "Epoch: 77/100... Step: 3730... Loss: 1.1753... Val Loss: 2.8337\n",
            "Epoch: 77/100... Step: 3740... Loss: 1.1631... Val Loss: 2.7996\n",
            "Epoch: 77/100... Step: 3750... Loss: 1.1411... Val Loss: 2.9091\n",
            "Epoch: 77/100... Step: 3760... Loss: 1.1447... Val Loss: 2.8872\n",
            "Epoch: 77/100... Step: 3770... Loss: 1.1595... Val Loss: 2.8206\n",
            "Epoch: 78/100... Step: 3780... Loss: 1.1393... Val Loss: 2.8085\n",
            "Epoch: 78/100... Step: 3790... Loss: 1.1526... Val Loss: 2.8419\n",
            "Epoch: 78/100... Step: 3800... Loss: 1.1439... Val Loss: 2.8848\n",
            "Epoch: 78/100... Step: 3810... Loss: 1.1586... Val Loss: 2.8937\n",
            "Epoch: 78/100... Step: 3820... Loss: 1.1763... Val Loss: 2.7910\n",
            "Epoch: 79/100... Step: 3830... Loss: 1.1461... Val Loss: 2.8385\n",
            "Epoch: 79/100... Step: 3840... Loss: 1.1616... Val Loss: 2.8537\n",
            "Epoch: 79/100... Step: 3850... Loss: 1.1492... Val Loss: 2.8676\n",
            "Epoch: 79/100... Step: 3860... Loss: 1.1547... Val Loss: 2.9386\n",
            "Epoch: 79/100... Step: 3870... Loss: 1.1507... Val Loss: 2.8639\n",
            "Epoch: 80/100... Step: 3880... Loss: 1.1335... Val Loss: 2.8125\n",
            "Epoch: 80/100... Step: 3890... Loss: 1.1097... Val Loss: 2.8564\n",
            "Epoch: 80/100... Step: 3900... Loss: 1.1666... Val Loss: 2.8609\n",
            "Epoch: 80/100... Step: 3910... Loss: 1.1448... Val Loss: 2.9559\n",
            "Epoch: 80/100... Step: 3920... Loss: 1.1622... Val Loss: 2.8504\n",
            "Epoch: 81/100... Step: 3930... Loss: 1.1168... Val Loss: 2.8408\n",
            "Epoch: 81/100... Step: 3940... Loss: 1.1375... Val Loss: 2.8713\n",
            "Epoch: 81/100... Step: 3950... Loss: 1.0945... Val Loss: 2.8657\n",
            "Epoch: 81/100... Step: 3960... Loss: 1.1578... Val Loss: 2.9440\n",
            "Epoch: 82/100... Step: 3970... Loss: 1.1584... Val Loss: 2.8637\n",
            "Epoch: 82/100... Step: 3980... Loss: 1.1375... Val Loss: 2.8628\n",
            "Epoch: 82/100... Step: 3990... Loss: 1.1352... Val Loss: 2.8586\n",
            "Epoch: 82/100... Step: 4000... Loss: 1.1446... Val Loss: 2.9153\n",
            "Epoch: 82/100... Step: 4010... Loss: 1.1542... Val Loss: 2.9477\n",
            "Epoch: 83/100... Step: 4020... Loss: 1.1282... Val Loss: 2.9204\n",
            "Epoch: 83/100... Step: 4030... Loss: 1.1463... Val Loss: 2.8545\n",
            "Epoch: 83/100... Step: 4040... Loss: 1.1087... Val Loss: 2.8646\n",
            "Epoch: 83/100... Step: 4050... Loss: 1.1057... Val Loss: 2.8546\n",
            "Epoch: 83/100... Step: 4060... Loss: 1.1370... Val Loss: 2.9354\n",
            "Epoch: 84/100... Step: 4070... Loss: 1.1409... Val Loss: 2.9276\n",
            "Epoch: 84/100... Step: 4080... Loss: 1.1301... Val Loss: 2.8875\n",
            "Epoch: 84/100... Step: 4090... Loss: 1.1127... Val Loss: 2.8873\n",
            "Epoch: 84/100... Step: 4100... Loss: 1.1381... Val Loss: 2.9023\n",
            "Epoch: 84/100... Step: 4110... Loss: 1.1115... Val Loss: 2.9752\n",
            "Epoch: 85/100... Step: 4120... Loss: 1.1355... Val Loss: 2.9500\n",
            "Epoch: 85/100... Step: 4130... Loss: 1.1343... Val Loss: 2.8822\n",
            "Epoch: 85/100... Step: 4140... Loss: 1.1424... Val Loss: 2.9282\n",
            "Epoch: 85/100... Step: 4150... Loss: 1.1241... Val Loss: 2.9130\n",
            "Epoch: 85/100... Step: 4160... Loss: 1.1045... Val Loss: 2.9831\n",
            "Epoch: 86/100... Step: 4170... Loss: 1.0960... Val Loss: 2.9533\n",
            "Epoch: 86/100... Step: 4180... Loss: 1.1076... Val Loss: 2.9099\n",
            "Epoch: 86/100... Step: 4190... Loss: 1.1252... Val Loss: 2.9601\n",
            "Epoch: 86/100... Step: 4200... Loss: 1.1451... Val Loss: 2.9490\n",
            "Epoch: 86/100... Step: 4210... Loss: 1.0961... Val Loss: 2.9645\n",
            "Epoch: 87/100... Step: 4220... Loss: 1.1205... Val Loss: 2.9661\n",
            "Epoch: 87/100... Step: 4230... Loss: 1.1121... Val Loss: 2.9364\n",
            "Epoch: 87/100... Step: 4240... Loss: 1.0971... Val Loss: 2.9727\n",
            "Epoch: 87/100... Step: 4250... Loss: 1.1059... Val Loss: 2.9642\n",
            "Epoch: 87/100... Step: 4260... Loss: 1.0952... Val Loss: 2.9610\n",
            "Epoch: 88/100... Step: 4270... Loss: 1.0906... Val Loss: 2.9588\n",
            "Epoch: 88/100... Step: 4280... Loss: 1.1108... Val Loss: 2.9535\n",
            "Epoch: 88/100... Step: 4290... Loss: 1.0953... Val Loss: 2.9942\n",
            "Epoch: 88/100... Step: 4300... Loss: 1.1031... Val Loss: 2.9677\n",
            "Epoch: 88/100... Step: 4310... Loss: 1.1183... Val Loss: 2.9882\n",
            "Epoch: 89/100... Step: 4320... Loss: 1.1026... Val Loss: 2.9618\n",
            "Epoch: 89/100... Step: 4330... Loss: 1.1245... Val Loss: 2.9324\n",
            "Epoch: 89/100... Step: 4340... Loss: 1.1023... Val Loss: 2.9949\n",
            "Epoch: 89/100... Step: 4350... Loss: 1.1138... Val Loss: 2.9859\n",
            "Epoch: 89/100... Step: 4360... Loss: 1.0836... Val Loss: 2.9901\n",
            "Epoch: 90/100... Step: 4370... Loss: 1.0842... Val Loss: 2.9956\n",
            "Epoch: 90/100... Step: 4380... Loss: 1.0815... Val Loss: 2.9875\n",
            "Epoch: 90/100... Step: 4390... Loss: 1.0930... Val Loss: 3.0051\n",
            "Epoch: 90/100... Step: 4400... Loss: 1.1070... Val Loss: 2.9975\n",
            "Epoch: 90/100... Step: 4410... Loss: 1.0995... Val Loss: 3.0274\n",
            "Epoch: 91/100... Step: 4420... Loss: 1.0822... Val Loss: 3.0112\n",
            "Epoch: 91/100... Step: 4430... Loss: 1.0901... Val Loss: 3.0012\n",
            "Epoch: 91/100... Step: 4440... Loss: 1.0722... Val Loss: 3.0171\n",
            "Epoch: 91/100... Step: 4450... Loss: 1.1165... Val Loss: 3.0208\n",
            "Epoch: 92/100... Step: 4460... Loss: 1.1044... Val Loss: 3.0091\n",
            "Epoch: 92/100... Step: 4470... Loss: 1.0785... Val Loss: 2.9974\n",
            "Epoch: 92/100... Step: 4480... Loss: 1.0970... Val Loss: 2.9856\n",
            "Epoch: 92/100... Step: 4490... Loss: 1.0967... Val Loss: 2.9695\n",
            "Epoch: 92/100... Step: 4500... Loss: 1.1187... Val Loss: 3.0027\n",
            "Epoch: 93/100... Step: 4510... Loss: 1.0819... Val Loss: 3.0018\n",
            "Epoch: 93/100... Step: 4520... Loss: 1.1091... Val Loss: 3.0058\n",
            "Epoch: 93/100... Step: 4530... Loss: 1.0687... Val Loss: 3.0059\n",
            "Epoch: 93/100... Step: 4540... Loss: 1.0890... Val Loss: 3.0137\n",
            "Epoch: 93/100... Step: 4550... Loss: 1.0836... Val Loss: 3.0105\n",
            "Epoch: 94/100... Step: 4560... Loss: 1.0995... Val Loss: 3.0276\n",
            "Epoch: 94/100... Step: 4570... Loss: 1.0926... Val Loss: 2.9947\n",
            "Epoch: 94/100... Step: 4580... Loss: 1.0614... Val Loss: 3.0303\n",
            "Epoch: 94/100... Step: 4590... Loss: 1.1098... Val Loss: 3.0701\n",
            "Epoch: 94/100... Step: 4600... Loss: 1.0769... Val Loss: 3.0441\n",
            "Epoch: 95/100... Step: 4610... Loss: 1.0899... Val Loss: 3.0154\n",
            "Epoch: 95/100... Step: 4620... Loss: 1.0935... Val Loss: 2.9787\n",
            "Epoch: 95/100... Step: 4630... Loss: 1.0882... Val Loss: 3.0260\n",
            "Epoch: 95/100... Step: 4640... Loss: 1.0824... Val Loss: 3.0250\n",
            "Epoch: 95/100... Step: 4650... Loss: 1.0794... Val Loss: 3.0096\n",
            "Epoch: 96/100... Step: 4660... Loss: 1.0637... Val Loss: 3.0560\n",
            "Epoch: 96/100... Step: 4670... Loss: 1.0773... Val Loss: 2.9826\n",
            "Epoch: 96/100... Step: 4680... Loss: 1.0827... Val Loss: 3.0723\n",
            "Epoch: 96/100... Step: 4690... Loss: 1.0971... Val Loss: 3.0465\n",
            "Epoch: 96/100... Step: 4700... Loss: 1.0562... Val Loss: 3.0193\n",
            "Epoch: 97/100... Step: 4710... Loss: 1.0699... Val Loss: 3.0756\n",
            "Epoch: 97/100... Step: 4720... Loss: 1.0714... Val Loss: 2.9948\n",
            "Epoch: 97/100... Step: 4730... Loss: 1.0604... Val Loss: 3.0728\n",
            "Epoch: 97/100... Step: 4740... Loss: 1.0631... Val Loss: 3.0381\n",
            "Epoch: 97/100... Step: 4750... Loss: 1.0694... Val Loss: 3.0241\n",
            "Epoch: 98/100... Step: 4760... Loss: 1.0616... Val Loss: 3.0852\n",
            "Epoch: 98/100... Step: 4770... Loss: 1.0626... Val Loss: 3.0311\n",
            "Epoch: 98/100... Step: 4780... Loss: 1.0677... Val Loss: 3.0474\n",
            "Epoch: 98/100... Step: 4790... Loss: 1.0867... Val Loss: 3.0543\n",
            "Epoch: 98/100... Step: 4800... Loss: 1.0873... Val Loss: 3.0238\n",
            "Epoch: 99/100... Step: 4810... Loss: 1.0578... Val Loss: 3.1168\n",
            "Epoch: 99/100... Step: 4820... Loss: 1.0765... Val Loss: 3.0349\n",
            "Epoch: 99/100... Step: 4830... Loss: 1.0630... Val Loss: 3.0591\n",
            "Epoch: 99/100... Step: 4840... Loss: 1.0984... Val Loss: 3.0572\n",
            "Epoch: 99/100... Step: 4850... Loss: 1.0641... Val Loss: 3.0483\n",
            "Epoch: 100/100... Step: 4860... Loss: 1.0613... Val Loss: 3.1220\n",
            "Epoch: 100/100... Step: 4870... Loss: 1.0408... Val Loss: 3.0740\n",
            "Epoch: 100/100... Step: 4880... Loss: 1.0885... Val Loss: 3.1136\n",
            "Epoch: 100/100... Step: 4890... Loss: 1.0835... Val Loss: 3.1102\n",
            "Epoch: 100/100... Step: 4900... Loss: 1.0788... Val Loss: 3.0179\n"
          ]
        }
      ],
      "source": [
        "#TO_DO definir hiperparámetros del entrenamiento\n",
        "batch_size = 64\n",
        "seq_length = 128\n",
        "n_epochs = 100 # empezar con un número pequeño al inicio para verificar el comportamiento del algoritmo\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P8lKI9CmKUb"
      },
      "source": [
        "# Parte 2\n",
        "\n",
        "## Guardar el modelo\n",
        "\n",
        "Luego de entrenar, vamos a guardar el modelo de manera que podamos cargarlo luego si lo necesitamos.\n",
        "A continuación, veamos la arquitectura del modelo y luego de eso lo guardamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O7WOmRmGdMsL",
        "outputId": "d2d230b0-9bff-4d7b-ee8a-18c825808289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('lstm.weight_ih_l0',\n",
              "              tensor([[-0.0856,  0.0433, -0.0042,  ...,  0.5023,  0.0773,  0.0246],\n",
              "                      [-0.0705, -0.0385,  0.0328,  ..., -0.1021, -0.1255,  0.1607],\n",
              "                      [ 0.0843, -0.0299, -0.0303,  ...,  0.0956,  0.1725,  0.4327],\n",
              "                      ...,\n",
              "                      [-0.2728,  0.0023, -0.0028,  ..., -0.2996,  0.0974,  0.0850],\n",
              "                      [-0.2461,  0.0114, -0.0012,  ..., -0.1068, -0.1032,  0.1914],\n",
              "                      [ 0.3577,  0.0195, -0.0333,  ..., -0.0178,  0.0283, -0.0805]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.weight_hh_l0',\n",
              "              tensor([[ 0.0965, -0.0679,  0.0131,  ..., -0.0068, -0.1385,  0.1106],\n",
              "                      [ 0.0557, -0.0522,  0.0799,  ...,  0.1387, -0.1215,  0.0583],\n",
              "                      [ 0.0517,  0.0506, -0.0236,  ..., -0.0551, -0.0771, -0.0836],\n",
              "                      ...,\n",
              "                      [ 0.2478,  0.0369,  0.0353,  ..., -0.0303,  0.0778,  0.0800],\n",
              "                      [-0.1803,  0.1301,  0.1630,  ..., -0.3313, -0.1539,  0.1153],\n",
              "                      [ 0.1286, -0.0097,  0.0609,  ...,  0.1664, -0.1024, -0.1631]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.bias_ih_l0',\n",
              "              tensor([0.1369, 0.0416, 0.0521,  ..., 0.0716, 0.0534, 0.0236], device='cuda:0')),\n",
              "             ('lstm.bias_hh_l0',\n",
              "              tensor([0.1740, 0.0540, 0.0852,  ..., 0.1269, 0.0588, 0.0530], device='cuda:0')),\n",
              "             ('lstm.weight_ih_l1',\n",
              "              tensor([[ 0.3402,  0.0900, -0.3218,  ..., -0.0430, -0.0388, -0.1478],\n",
              "                      [-0.2819, -0.0462, -0.0160,  ..., -0.0278,  0.0039,  0.4457],\n",
              "                      [-0.2066,  0.1927,  0.2869,  ..., -0.2267, -0.2885, -0.0133],\n",
              "                      ...,\n",
              "                      [-0.2009,  0.1675,  0.1258,  ..., -0.2063, -0.0318,  0.2727],\n",
              "                      [ 0.0752,  0.3783,  0.0698,  ..., -0.0207, -0.0119, -0.1422],\n",
              "                      [ 0.3490, -0.0254, -0.0432,  ..., -0.2654,  0.0206,  0.1002]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.weight_hh_l1',\n",
              "              tensor([[-0.3402, -0.3080,  0.0533,  ..., -0.0521, -0.0260, -0.0160],\n",
              "                      [-0.0047,  0.0723,  0.1485,  ..., -0.0421, -0.1656,  0.2358],\n",
              "                      [ 0.0552, -0.2143,  0.1781,  ..., -0.0408, -0.0118, -0.2124],\n",
              "                      ...,\n",
              "                      [ 0.1279, -0.0733,  0.0113,  ...,  0.0817,  0.0283,  0.1627],\n",
              "                      [-0.1033,  0.4209, -0.1089,  ...,  0.0238,  0.0747, -0.0958],\n",
              "                      [-0.0392, -0.4913,  0.0178,  ..., -0.0883, -0.1152, -0.0645]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.bias_ih_l1',\n",
              "              tensor([0.1266, 0.2226, 0.0157,  ..., 0.0804, 0.0524, 0.1995], device='cuda:0')),\n",
              "             ('lstm.bias_hh_l1',\n",
              "              tensor([0.1143, 0.1570, 0.0445,  ..., 0.0214, 0.0225, 0.2080], device='cuda:0')),\n",
              "             ('lstm.weight_ih_l2',\n",
              "              tensor([[ 0.0106,  0.0086, -0.0375,  ...,  0.0587,  0.0176,  0.0661],\n",
              "                      [-0.0090, -0.0954,  0.0468,  ..., -0.0140,  0.0176,  0.0278],\n",
              "                      [ 0.0580, -0.1251, -0.1690,  ..., -0.0331,  0.0933, -0.1172],\n",
              "                      ...,\n",
              "                      [ 0.0548, -0.0257,  0.1495,  ...,  0.0748, -0.0752,  0.0882],\n",
              "                      [-0.0216, -0.1138, -0.0407,  ..., -0.0224, -0.0654, -0.0094],\n",
              "                      [-0.1433, -0.0175, -0.1113,  ..., -0.0121,  0.0113, -0.2547]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.weight_hh_l2',\n",
              "              tensor([[ 0.5454,  0.0192,  0.2583,  ...,  0.1744, -0.0516,  0.0349],\n",
              "                      [-0.1102, -0.1201, -0.0297,  ..., -0.0455, -0.1401,  0.1060],\n",
              "                      [ 0.0760,  0.1294,  0.1963,  ..., -0.0339, -0.0113,  0.0254],\n",
              "                      ...,\n",
              "                      [ 0.2602, -0.0332,  0.0191,  ..., -0.1824, -0.0741, -0.1770],\n",
              "                      [ 0.2563,  0.2530, -0.0216,  ..., -0.1210, -0.2636, -0.1445],\n",
              "                      [ 0.1062,  0.1001,  0.2782,  ..., -0.1642, -0.1033, -0.0889]],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.bias_ih_l2',\n",
              "              tensor([-0.0586,  0.0008,  0.0104,  ...,  0.0129,  0.0409, -0.0580],\n",
              "                     device='cuda:0')),\n",
              "             ('lstm.bias_hh_l2',\n",
              "              tensor([-0.0706,  0.0432,  0.0032,  ...,  0.0253, -0.0064, -0.0044],\n",
              "                     device='cuda:0')),\n",
              "             ('fc.weight',\n",
              "              tensor([[ 0.0311,  0.0919, -0.0547,  ...,  0.0243, -0.0795,  0.0823],\n",
              "                      [-0.1198, -0.0820,  0.0901,  ...,  0.0167,  0.3760, -0.1232],\n",
              "                      [-0.1348, -0.0630,  0.1132,  ...,  0.0167,  0.3540, -0.1094],\n",
              "                      ...,\n",
              "                      [ 0.0845, -0.2659, -0.0323,  ...,  0.0204,  0.0658, -0.1164],\n",
              "                      [-0.2411, -0.2444, -0.0871,  ...,  0.1112,  0.1955, -0.0970],\n",
              "                      [-0.0454, -0.0292,  0.0609,  ...,  0.1151, -0.0460, -0.1949]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc.bias',\n",
              "              tensor([ 0.1789, -0.0577, -0.0915, -0.1125, -0.0660, -0.1675, -0.0390, -0.0637,\n",
              "                      -0.1109, -0.1072, -0.0396, -0.0321, -0.1527, -0.0575, -0.0953, -0.0856,\n",
              "                      -0.0834, -0.2459, -0.0587, -0.1078, -0.1893, -0.2618, -0.1611, -0.0450,\n",
              "                      -0.1822, -0.4710, -0.0665, -0.2695,  0.1226, -0.1343, -0.0873, -0.1314,\n",
              "                      -0.0911,  0.2171, -0.0678, -0.2726, -0.1598, -0.0450,  0.0635, -0.0950,\n",
              "                      -0.1158, -0.0996, -0.1529, -0.1317, -0.1313, -0.0603, -0.1404, -0.0909,\n",
              "                      -0.0786,  0.0061, -0.1051, -0.0677, -0.1199, -0.1847, -0.0994, -0.0971,\n",
              "                      -0.0922, -0.0292, -0.1796, -0.1247,  0.0509,  0.0219,  0.1660, -0.1890,\n",
              "                      -0.0842, -0.0049, -0.0778, -0.2808, -0.0557, -0.1141,  0.1340,  0.1161,\n",
              "                       0.1791, -0.0199, -0.0454, -0.0953, -0.0011,  0.0168, -0.1231, -0.3058,\n",
              "                      -0.1172, -0.1245, -0.0615, -0.1350, -0.1264, -0.1274, -0.1505, -0.1442,\n",
              "                      -0.0129, -0.0800,  0.0925, -0.0131, -0.1148, -0.0962, -0.2609, -0.1548,\n",
              "                       0.1164, -0.1641, -0.0412, -0.1419, -0.1954, -0.0710, -0.0631, -0.2386,\n",
              "                      -0.0514, -0.0691, -0.0741,  0.0349, -0.1157, -0.0412, -0.0821],\n",
              "                     device='cuda:0'))])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TYSkUiMHo-QY"
      },
      "outputs": [],
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTa2QC4nox9X"
      },
      "source": [
        "---\n",
        "## Haciendo predicciones\n",
        "\n",
        "Ahora que el modelo está entrenado, queremos usarlo para hacer predicciones sobre los caracteres siguientes. Para obtener una muestra de de texto,  le pasamos un caracter y dejamos que la red prediga el siguiente caracter. Luego tomamos dicho caracter, lo pasamos a la red y obtenemos el siguiente caracter predicho. Seguimos haciendo lo mismo hasta generar una porción de texto.\n",
        "\n",
        "\n",
        "### Observación\n",
        "\n",
        "La salida de nuestra red neuronal viene de una capa lineal (fully-connected) y produce como salida una distribución de puntajes para el siguiente caracter.\n",
        "\n",
        "Para obtener el caracter siguiente, aplicamos una función softmax, que nos da una distribución de probabilidad.\n",
        "\n",
        "### Top K (los $K$ más probables)\n",
        "Nuestras predicciones vienen de una distribución de probabilidad sobre todos los caracteres posibles. Podemos generar las predicciones considerando solamente los $K$ caracteres más probables. Esto prevendrá a la red de arrojar caracteres absurdos, pero permitiendo introducir algo de aleatoriedad en el texto generado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cFzGFJ-HoxJy"
      },
      "outputs": [],
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "\n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "\n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAYmNKNxtEXH"
      },
      "source": [
        "### Preparar la red y generar el texto\n",
        "\n",
        "Típicamente se querrá preparar la red con unos primeros caracteres (estos los definimos en el parámetro 'prime'), de manera que se pueda construir un primer estado $h_0$.  De lo contrario la red comenzará generando caracteres aleatoriamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "0BL-HtUIdMsK"
      },
      "outputs": [],
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval() # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28x6T8m7uD7H",
        "outputId": "f3e78228-d34d-41d0-cf8a-770f807bf612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annacen), a pesar de casa de los cocodrilosos, que aprovechando el aroma del pobre Materra. El pobre Guannes, contempla\n",
            "el callareo escarrritando, de seguro en la punta\n",
            "de una antigua, que eran desnudos, escurriendo las pulidras y las\n",
            "heridas de arrependible al monte de un color que le dure estas manas en en\n",
            "campo más presamente al altar. Para su celestial madrina, el medio de aquella torde de su vida; y el mundo cubierta de amarle es un corrón, que duró, pasaba el mármol que esparcía.\n",
            "\n",
            "El pueblo se acodaba deserado de este sol, casi levantada y rico, sin cabellos blancos; de cadera, aparecía a los cuales descomenzando por la puerta de los\n",
            "cabellos, a lo lindo de la palma; a los cuales era un profundo cuatro por la cabeza del mostrador. A poco,\n",
            "como en esa semana como en su pasado\n",
            "y solitario\n",
            "que le\n",
            "había sido los ojos y de lentaje, en\n",
            "el comedor de medios, de contigua friolesta si toda le encaje... Y sin comprenderle, y caminó, para su\n",
            "comprender don Alino. En esto,\n",
            "conservaba los desesper\n"
          ]
        }
      ],
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HB-PnUAnW6H"
      },
      "source": [
        "### Conclusión:\n",
        "\n",
        "**TO_DO** ¿Qué libro eligió? EL texto generado tiene sentido gramaticalmente? ¿El estilo del texto es similar al de su libro? ¿Por qué?  Si alguna de estas respuestas es negativa, considere diferentes hiperparámetros para su modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLSjStfOvYGe"
      },
      "source": [
        "Elegí Adan y Eva en español, gramaticalmente se ve bien, el estilo es similar,es similar porque estamos trabajando sobre el texto y hace una predicción sobre este. Sin embargo, no sé que tanta coherencia tenga\n",
        "(Dado es caracter por caracter es dificil obtener coherencia)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
